import shap
import matplotlib.pyplot as plt

def explain_model_shap(model, X, feature_names=None, max_display=15):
    """
    Calcula e imprime interpretabilidad del modelo usando SHAP.
    
    Par√°metros:
    - model: modelo ya entrenado (LightGBM, XGBoost, etc.)
    - X: datos ya preprocesados (np.array o DataFrame)
    - feature_names: nombres de las features si X es array
    - max_display: cu√°ntas features mostrar en los gr√°ficos
    
    Muestra:
    - Summary plot (bar y beeswarm)
    """
    print("‚úÖ Calculando valores SHAP...")
    
    # Crear el explainer
    explainer = shap.Explainer(model)
    shap_values = explainer(X)

    # Nombre de features si es array
    if feature_names is not None:
        shap_values.feature_names = feature_names

    # Gr√°fico tipo summary (beeswarm)
    print("üìä SHAP Summary (Beeswarm):")
    shap.plots.beeswarm(shap_values, max_display=max_display, show=False)
    #guardar gr√°ficos
    plt.tight_layout()  # Ajustar layout para evitar superposici√≥n
    plt.savefig("ml_model_evaluator/results/shap_summary_plot.png", dpi=700)  # .png, .pdf tambi√©n son soportados
    plt.clf()  # Limpiar figura para evitar superposici√≥n de gr√°ficos

    # Gr√°fico tipo bar (media de importancia absoluta)
    print("üìä SHAP Feature Importance (Bar):")
    shap.plots.bar(shap_values, max_display=max_display, show=False)
    #guardar gr√°ficos
    plt.tight_layout()  # Ajustar layout para evitar superposici√≥n
    plt.savefig("ml_model_evaluator/results/shap_bar_plot.png", dpi=700)  # .png, .pdf tambi√©n son soportados
    print("‚úÖ Gr√°ficos SHAP guardados en resultados.")

